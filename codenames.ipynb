{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# color and word detection\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "import cv2\n",
    "import pytesseract\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# clue generator\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from itertools import zip_longest\n",
    "from IPython.display import HTML\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.data import find\n",
    "nltk.download('word2vec_sample')\n",
    "\n",
    "# in case we run into download issues\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Card Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color card images\n",
    "color_card = cv2.imread('assets/color_card.jpg')\n",
    "cropped_color_card = color_card[300:1400, 275:1400]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block is used to try and remove brightness/lighting problems\n",
    "def preprocess_img(img):\n",
    "  # Convert the image to LAB color space\n",
    "  lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "  # Split the LAB image into its 3 channels\n",
    "  l, a, b = cv2.split(lab)\n",
    "\n",
    "  # Apply CLAHE to the L channel\n",
    "  clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "  cl = clahe.apply(l)\n",
    "\n",
    "  # increase brightnes of iamge\n",
    "  limg = cv2.merge((cl,a,b))\n",
    "\n",
    "  # Convert the image back to BGR color space\n",
    "  processed_img = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "  return processed_img \n",
    "\n",
    "processed_color_card = preprocess_img(cropped_color_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove noise from the image\n",
    "kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "\n",
    "gray_orig = cv2.cvtColor(processed_color_card.copy(), cv2.COLOR_BGR2GRAY)\n",
    "blur = cv2.GaussianBlur(gray_orig,(5,5),0)\n",
    "blur = cv2.filter2D(blur, -1, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approx color ranges\n",
    "lower_blue = np.array([60,100,50])\n",
    "upper_blue = np.array([140,255,255])\n",
    "\n",
    "lower_red = np.array([0,50,50])\n",
    "upper_red = np.array([60,255,255])\n",
    "\n",
    "lower_beige = np.array([0, 10, 120])\n",
    "upper_beige = np.array([40, 80, 255])\n",
    "\n",
    "# lower_black = np.array([0, 0, 0])\n",
    "# upper_black = np.array([179, 255, 30])\n",
    "\n",
    "# convert images to HSV standard\n",
    "red_img = cv2.cvtColor(processed_color_card.copy(), cv2.COLOR_BGR2HSV)\n",
    "blue_img = cv2.cvtColor(processed_color_card.copy(), cv2.COLOR_BGR2HSV)\n",
    "beige_img = cv2.cvtColor(processed_color_card.copy(), cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# find each individual color using color masks\n",
    "red_mask = cv2.inRange(red_img, lower_red, upper_red)\n",
    "blue_mask = cv2.inRange(blue_img, lower_blue, upper_blue)\n",
    "beige_mask = cv2.inRange(beige_img, lower_beige, upper_beige)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test finding of red squares\n",
    "plt.imshow(red_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test finding of blue squares\n",
    "plt.imshow(blue_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test finding of beige squares\n",
    "plt.imshow(beige_mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Contour Detection --> Detect Each Color Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contour detection section\n",
    "img_h,img_w = gray_orig.shape\n",
    "background_thresh = gray_orig[0][0]\n",
    "ADD_THRESH = 90\n",
    "total_thresh = background_thresh \n",
    "_,thresh_img = cv2.threshold(blur,total_thresh,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "plt.imshow(thresh_img, cmap=\"gray\", vmin=0, vmax=255)\n",
    "\n",
    "contours, hier = cv2.findContours(thresh_img,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "contours = [contour for contour,h in zip(contours,hier[0]) if h[3] == -1 and h[2] > -1]\n",
    "top_25_contours = sorted(contours, key=lambda x : cv2.contourArea(x) if cv2.contourArea(x) < (img_h * img_w)/25 else 0,reverse=True)[:25]\n",
    "\n",
    "\n",
    "# sort x and y later\n",
    "coords_and_index = []\n",
    "for i,contour in enumerate(top_25_contours):\n",
    "    x, y, _, _ = cv2.boundingRect(contour)\n",
    "    coords_and_index.append((i,x,y))\n",
    "\n",
    "\n",
    "# sort by y\n",
    "sorted_y = sorted(coords_and_index,key=lambda x:x[2])\n",
    "\n",
    "\n",
    "# sort by x\n",
    "for i in range(5):\n",
    "    sorted_y[5 * i:5* (i + 1)] = sorted(sorted_y[5 * i:5* (i + 1)], key=lambda x:x[1])\n",
    "top_25_sorted = [top_25_contours[i[0]] for i in sorted_y]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hightlight contours and label each square to check correctness\n",
    "print_img = cv2.cvtColor(thresh_img, cv2.COLOR_BGR2RGB)\n",
    "cv2.drawContours(print_img, top_25_contours, -1, (0, 255, 0), 3)\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "fontScale = 3\n",
    "color = (255, 0, 0)\n",
    "thickness = 5\n",
    "for i, place in enumerate(sorted_y):  \n",
    "    # print(i, place[1], place[2])\n",
    "    cv2.putText(print_img, str(i), (place[1] + 10,place[2] + 10), font, \n",
    "                   fontScale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "plt.imshow(print_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Color Detection --> Detect Color of each Square/Contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this stores the color of each respective square \n",
    "colors = []\n",
    "hsv_cropped_img = cv2.cvtColor(cropped_color_card.copy(), cv2.COLOR_BGR2HSV)\n",
    "\n",
    "lower_blue = np.array([60,100,50])\n",
    "upper_blue = np.array([140,255,255])\n",
    "\n",
    "lower_red = np.array([0,50,50])\n",
    "upper_red = np.array([60,255,255])\n",
    "\n",
    "lower_beige = np.array([0, 10, 120])\n",
    "upper_beige = np.array([40, 80, 255])\n",
    "\n",
    "lower_black = np.array([0, 0, 0])\n",
    "upper_black = np.array([179, 255, 30])\n",
    "\n",
    "for i, place in enumerate(sorted_y):\n",
    "    color = np.mean(hsv_cropped_img[place[2]:place[2]+150, place[1]:place[1]+150], axis=(0, 1))\n",
    "    # print(i, color)\n",
    "    if all(color <= upper_beige) and all(color >= lower_beige):\n",
    "        colors.append((i, \"beige\"))\n",
    "    elif all(color <= upper_black) and all(color >= lower_black):\n",
    "        colors.append((i, \"black\"))\n",
    "    elif all(color <= upper_blue) and all(color >= lower_blue):\n",
    "        colors.append((i, \"blue\"))\n",
    "    elif all(color <= upper_red) and all(color >= lower_red):\n",
    "        colors.append((i, \"red\"))\n",
    "    else:\n",
    "        colors.append((i, \"black\"))\n",
    "\n",
    "# print(colors)\n",
    "if len(colors) == 25:\n",
    "    print('Found All Squares')\n",
    "else:\n",
    "    print(\"May not have found all squares\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cards Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words(filepath):\n",
    "    img_board = cv2.imread(filepath)\n",
    "    img_board = cv2.rotate(img_board, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    img_board_gray = cv2.cvtColor(img_board, cv2.COLOR_BGR2GRAY)\n",
    "    img_h,img_w = img_board_gray.shape\n",
    "    background_thresh = img_board_gray[0][0]\n",
    "    ADD_THRESH = 90\n",
    "    blur = cv2.GaussianBlur(img_board_gray,(5,5),0)\n",
    "    total_thresh = background_thresh + ADD_THRESH\n",
    "    _,thresh_img = cv2.threshold(blur,total_thresh,255,cv2.THRESH_BINARY)\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7,7))\n",
    "    opening = cv2.morphologyEx(thresh_img, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    contours, hier = cv2.findContours(opening,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = [contour for contour,h in zip(contours,hier[0]) if h[3] == -1 and h[2] > -1]\n",
    "    top_25_contours = sorted(contours, key=lambda x : cv2.contourArea(x) if cv2.contourArea(x) < (img_h * img_w)/25 else 0,reverse=True)[:25]\n",
    "\n",
    "    # sort x and y later\n",
    "    coords_and_index = []\n",
    "    for i,contour in enumerate(top_25_contours):\n",
    "        x, y, _, _ = cv2.boundingRect(contour)\n",
    "        coords_and_index.append((i,x,y))\n",
    "    sorted_y = sorted(coords_and_index,key=lambda x:x[2])\n",
    "    for i in range(5):\n",
    "        sorted_y[5 * i:5* (i + 1)] = sorted(sorted_y[5 * i:5* (i + 1)], key=lambda x:x[1])\n",
    "    top_25_sorted = [top_25_contours[i[0]] for i in sorted_y]\n",
    "\n",
    "\n",
    "    def flattener(image, pts, w, h):\n",
    "        \"\"\"Flattens an image of a card into a top-down 200x300 perspective.\n",
    "        Returns the flattened, re-sized, grayed image.\n",
    "        See www.pyimagesearch.com/2014/08/25/4-point-opencv-getperspective-transform-example/\"\"\"\n",
    "        temp_rect = np.zeros((4,2), dtype = \"float32\")\n",
    "        \n",
    "        s = np.sum(pts, axis = 2)\n",
    "\n",
    "        tl = pts[np.argmin(s)]\n",
    "        br = pts[np.argmax(s)]\n",
    "\n",
    "        diff = np.diff(pts, axis = -1)\n",
    "        tr = pts[np.argmin(diff)]\n",
    "        bl = pts[np.argmax(diff)]\n",
    "\n",
    "        # Need to create an array listing points in order of\n",
    "        # [top left, top right, bottom right, bottom left]\n",
    "        # before doing the perspective transform\n",
    "\n",
    "        if w <= 0.8*h: # If card is vertically oriented\n",
    "            temp_rect[0] = tl\n",
    "            temp_rect[1] = tr\n",
    "            temp_rect[2] = br\n",
    "            temp_rect[3] = bl\n",
    "\n",
    "        if w >= 1.2*h: # If card is horizontally oriented\n",
    "            temp_rect[0] = bl\n",
    "            temp_rect[1] = tl\n",
    "            temp_rect[2] = tr\n",
    "            temp_rect[3] = br\n",
    "\n",
    "        # If the card is 'diamond' oriented, a different algorithm\n",
    "        # has to be used to identify which point is top left, top right\n",
    "        # bottom left, and bottom right.\n",
    "        \n",
    "        if w > 0.8*h and w < 1.2*h: #If card is diamond oriented\n",
    "            # If furthest left point is higher than furthest right point,\n",
    "            # card is tilted to the left.\n",
    "            if pts[1][0][1] <= pts[3][0][1]:\n",
    "                # If card is titled to the left, approxPolyDP returns points\n",
    "                # in this order: top right, top left, bottom left, bottom right\n",
    "                temp_rect[0] = pts[1][0] # Top left\n",
    "                temp_rect[1] = pts[0][0] # Top right\n",
    "                temp_rect[2] = pts[3][0] # Bottom right\n",
    "                temp_rect[3] = pts[2][0] # Bottom left\n",
    "\n",
    "            # If furthest left point is lower than furthest right point,\n",
    "            # card is tilted to the right\n",
    "            if pts[1][0][1] > pts[3][0][1]:\n",
    "                # If card is titled to the right, approxPolyDP returns points\n",
    "                # in this order: top left, bottom left, bottom right, top right\n",
    "                temp_rect[0] = pts[0][0] # Top left\n",
    "                temp_rect[1] = pts[3][0] # Top right\n",
    "                temp_rect[2] = pts[2][0] # Bottom right\n",
    "                temp_rect[3] = pts[1][0] # Bottom left\n",
    "                \n",
    "            \n",
    "        maxWidth = 200\n",
    "        maxHeight = 300\n",
    "\n",
    "        dst = np.array([[0,0],[maxWidth-1,0],[maxWidth-1,maxHeight-1],[0, maxHeight-1]], np.float32)\n",
    "        M = cv2.getPerspectiveTransform(temp_rect,dst)\n",
    "        warp = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
    "        warp = cv2.cvtColor(warp,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        return warp\n",
    "\n",
    "    def find_words(top_25_sorted):\n",
    "        words = []\n",
    "        for cont in top_25_sorted:\n",
    "            peri = cv2.arcLength(cont,True)\n",
    "            approx = cv2.approxPolyDP(cont,0.01*peri,True)\n",
    "            pts = np.float32(approx)\n",
    "            corner_pts = pts\n",
    "\n",
    "            x,y,w,h = cv2.boundingRect(cont)\n",
    "            width, height = w, h\n",
    "\n",
    "            average = np.sum(pts, axis=0)/len(pts)\n",
    "            cent_x = int(average[0][0])\n",
    "            cent_y = int(average[0][1])\n",
    "            center = [cent_x, cent_y]\n",
    "\n",
    "            warp = cv2.rotate(flattener(img_board, pts, w, h),cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "            cropped_img = warp[warp.shape[0]//2 + 20: warp.shape[0]-20, 20:warp.shape[1]-20]\n",
    "            blur = cv2.GaussianBlur(cropped_img, (3,3), 0)\n",
    "            contrast = cv2.convertScaleAbs(blur, alpha=1.3, beta=0)\n",
    "            thresh = cv2.threshold(contrast, 0, 255, cv2.THRESH_BINARY_INV  + cv2.THRESH_OTSU)[1]\n",
    "            words.append(pytesseract.image_to_string(thresh, lang='eng', config='--psm 6').strip().lower())\n",
    "        return words\n",
    "    words = find_words(top_25_sorted)\n",
    "    if len(set(words)) == 25:\n",
    "        print('success')\n",
    "    else:\n",
    "        print('fail, not all words may have been found')\n",
    "    return(words)\n",
    "\n",
    "words = read_words('assets/5x5.jpg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clue Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_words = []\n",
    "blue_words = []\n",
    "black_words = []\n",
    "beige_words = []\n",
    "for word, color in zip(words, colors):\n",
    "    if color[1] == 'red':\n",
    "        red_words.append(word)\n",
    "    elif color[1] == 'blue':\n",
    "        blue_words.append(word)\n",
    "    elif color[1] == 'black':\n",
    "        black_words.append(word)\n",
    "    elif color[1] == 'beige':\n",
    "        beige_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model - most common ~44k words\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = KeyedVectors.load_word2vec_format(\n",
    "    word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lava ('volcano', 'cave', 'ball') 0.5370753407478333\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "best_guess = \"\"\n",
    "best_words = []\n",
    "best_sim = 0\n",
    "\n",
    "# Filter words that are not in the vocab of the model\n",
    "def filter_words(words):\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            _ = model[word]\n",
    "            filtered_words.append(word)\n",
    "        except KeyError:\n",
    "            print(word, \"not in vocab\")\n",
    "            continue\n",
    "    return filtered_words\n",
    "\n",
    "filtered_red = filter_words(red_words)\n",
    "# filtered_blue = filter_words(blue_words)\n",
    "filtered_black = filter_words(black_words)\n",
    "# filtered_beige = filter_words(beige_words)\n",
    "\n",
    "# Return powerset of words from start len to end len\n",
    "def powerset(iterable,start,end):\n",
    "    s = list(iterable)\n",
    "    return list(chain.from_iterable(combinations(s, r) for r in range(start,end + 1)))\n",
    "\n",
    "# Search from 1 or 2 to 4 word combinations\n",
    "start = 1 if len(red_words) == 1 else 2\n",
    "possible_answers = powerset(filtered_red, start, 4)\n",
    "\n",
    "for possible_answer in possible_answers:\n",
    "    guesses = model.most_similar(positive=list(possible_answer), negative=filtered_black)\n",
    "\n",
    "    # Use stemming to filter out words that are from the same root word\n",
    "    final_guess = []\n",
    "    for guess in guesses:\n",
    "        final_guess = guess\n",
    "        valid_guess = True\n",
    "        for word in possible_answer:\n",
    "            if(ps.stem(word) == ps.stem(guess[0]) or word.lower() in guess[0].lower() or guess[0].lower() in word.lower()):\n",
    "                valid_guess = False\n",
    "                break\n",
    "        if valid_guess:\n",
    "            break\n",
    "\n",
    "    if final_guess[1] > best_sim:\n",
    "        print(guesses)\n",
    "        best_guess = final_guess[0]\n",
    "        best_words = possible_answer\n",
    "        best_sim = final_guess[1]\n",
    "\n",
    "print(best_guess, best_words, best_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-prac-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
