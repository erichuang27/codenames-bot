{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# color and word detection\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "# clue generator\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from itertools import zip_longest\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Card Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color card images\n",
    "color_card = cv2.imread('assets/color_card.jpg')\n",
    "color_card2 = cv2.imread('assets/color_card2.PNG')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block is used to try and remove brightness/lighting problems\n",
    "def preprocess_img(img):\n",
    "  # Convert the image to LAB color space\n",
    "  lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "  # Split the LAB image into its 3 channels\n",
    "  l, a, b = cv2.split(lab)\n",
    "\n",
    "  # Apply CLAHE to the L channel\n",
    "  clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "  cl = clahe.apply(l)\n",
    "\n",
    "  # increase brightnes of iamge\n",
    "  limg = cv2.merge((cl,a,b))\n",
    "\n",
    "  # Convert the image back to BGR color space\n",
    "  processed_img = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "  return processed_img \n",
    "\n",
    "processed_color_card = preprocess_img(color_card[200:1400, 275:1400])\n",
    "processed_color_card2 = preprocess_img(color_card2[250:1750, 250:1750])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color ranges\n",
    "lower_blue = np.array([100,150,50])\n",
    "upper_blue = np.array([140,255,255])\n",
    "\n",
    "lower_red = np.array([0,50,50])\n",
    "upper_red = np.array([10,255,255])\n",
    "\n",
    "lower_beige = np.array([20, 10, 150])\n",
    "upper_beige = np.array([40, 70, 255])\n",
    "\n",
    "lower_gray = np.array([50])\n",
    "upper_gray = np.array([150])\n",
    "\n",
    "# convert images to HSV standard\n",
    "red_img = cv2.cvtColor(processed_color_card2.copy(), cv2.COLOR_BGR2HSV)\n",
    "blue_img = cv2.cvtColor(processed_color_card2.copy(), cv2.COLOR_BGR2HSV)\n",
    "beige_img = cv2.cvtColor(processed_color_card2.copy(), cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# find each individual color using color masks\n",
    "red_mask = cv2.inRange(red_img, lower_red, upper_red)\n",
    "blue_mask = cv2.inRange(blue_img, lower_blue, upper_blue)\n",
    "beige_mask = cv2.inRange(beige_img, lower_beige, upper_beige)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test finding of red squares\n",
    "plt.imshow(red_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test finding of blue squares\n",
    "plt.imshow(blue_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test finding of beige squares\n",
    "plt.imshow(beige_mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Contour Detection --> Detect Each Color Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contour detection section\n",
    "gray_color_card2 = cv2.cvtColor(processed_color_card2.copy(), cv2.COLOR_BGR2GRAY)\n",
    "img_h,img_w = gray_color_card2.shape\n",
    "background_thresh = gray_color_card2[0][0]\n",
    "blur = cv2.GaussianBlur(gray_color_card2,(5,5),0)\n",
    "total_thresh = background_thresh\n",
    "_,thresh_img = cv2.threshold(blur,total_thresh,255,cv2.THRESH_BINARY)\n",
    "plt.imshow(thresh_img, cmap=\"gray\", vmin=0, vmax=255)\n",
    "contours, hier = cv2.findContours(thresh_img,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "top_25_contours = sorted(contours, key=lambda x : cv2.contourArea(x) if cv2.contourArea(x) < (img_h * img_w)/25 else 0,reverse=True)[:25]\n",
    "\n",
    "# sort x and y later\n",
    "coords_and_index = []\n",
    "for i,contour in enumerate(top_25_contours):\n",
    "    x, y, _, _ = cv2.boundingRect(contour)\n",
    "    coords_and_index.append((i,x,y))\n",
    "\n",
    "\n",
    "# sort by y\n",
    "sorted_y = sorted(coords_and_index,key=lambda x:x[2])\n",
    "\n",
    "\n",
    "# sort by x\n",
    "for i in range(5):\n",
    "    sorted_y[5 * i:5* (i + 1)] = sorted(sorted_y[5 * i:5* (i + 1)], key=lambda x:x[1])\n",
    "top_25_sorted = [top_25_contours[i[0]] for i in sorted_y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hightlight contours and label each square to check correctness\n",
    "print_img = cv2.cvtColor(thresh_img, cv2.COLOR_BGR2RGB)\n",
    "cv2.drawContours(print_img, top_25_contours, -1, (0, 255, 0), 3)\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "fontScale = 3\n",
    "color = (255, 0, 0)\n",
    "thickness = 5\n",
    "for i, place in enumerate(sorted_y):  \n",
    "    # print(i, place[1], place[2])\n",
    "    cv2.putText(print_img, str(i), (place[1] + 10,place[2] + 10), font, \n",
    "                   fontScale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "plt.imshow(print_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Color Detection --> Detect Color of each Square/Contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this stores the color of each respective square\n",
    "colors = []\n",
    "hsv_cropped_img = cv2.cvtColor(processed_color_card2[250:1750, 250:1750].copy(), cv2.COLOR_BGR2HSV)\n",
    "\n",
    "lower_blue = np.array([100,150,50])\n",
    "upper_blue = np.array([140,255,255])\n",
    "\n",
    "lower_red = np.array([0,50,50])\n",
    "upper_red = np.array([10,255,255])\n",
    "\n",
    "lower_beige = np.array([20, 10, 150])\n",
    "upper_beige = np.array([40, 80, 255])\n",
    "\n",
    "for i, place in enumerate(sorted_y):\n",
    "    # get color in middle of square\n",
    "    color = hsv_cropped_img[place[2]+90, place[1]+90]\n",
    "    if all(color <= upper_red) and all(color >= lower_red):\n",
    "        colors.append((i, \"red\"))\n",
    "    elif all(color <= upper_blue) and all(color >= lower_blue):\n",
    "        colors.append((i, \"blue\"))\n",
    "    elif all(color <= upper_beige) and all(color >= lower_beige):\n",
    "        colors.append((i, \"beige\"))\n",
    "    else:\n",
    "        colors.append((i, \"black\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cards Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contour Detection --> Find each word card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_board = cv2.imread('assets/5x5_2.jpg')\n",
    "word_board = cv2.rotate(word_board, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "word_board_gray = cv2.cvtColor(word_board, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "img_h,img_w = word_board_gray.shape\n",
    "background_thresh = word_board_gray[0][0]\n",
    "ADD_THRESH = 90\n",
    "blur = cv2.GaussianBlur(word_board_gray,(5,5),0)\n",
    "total_thresh = background_thresh + ADD_THRESH\n",
    "_,thresh_img = cv2.threshold(blur,total_thresh,255,cv2.THRESH_BINARY)\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7,7))\n",
    "opening = cv2.morphologyEx(thresh_img, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "contours, hier = cv2.findContours(opening,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "contours = [contour for contour,h in zip(contours,hier[0]) if h[3] == -1 and h[2] > -1]\n",
    "top_25_contours = sorted(contours, key=lambda x : cv2.contourArea(x) if cv2.contourArea(x) < (img_h * img_w)/25 else 0,reverse=True)[:25]\n",
    "\n",
    "# sort x and y later\n",
    "coords_and_index = []\n",
    "for i,contour in enumerate(top_25_contours):\n",
    "    x, y, _, _ = cv2.boundingRect(contour)\n",
    "    coords_and_index.append((i,x,y))\n",
    "# print(coords_and_index)\n",
    "\n",
    "sorted_y = sorted(coords_and_index,key=lambda x:x[2])\n",
    "# print(sorted_y)\n",
    "\n",
    "for i in range(5):\n",
    "    sorted_y[5 * i:5* (i + 1)] = sorted(sorted_y[5 * i:5* (i + 1)], key=lambda x:x[1])\n",
    "top_25_sorted = [top_25_contours[i[0]] for i in sorted_y]\n",
    "# print(sorted_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Original Board***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Original Board', cv2.resize(word_board, (960, 540)))\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Thresholded Image***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Thresholded Image', cv2.resize(opening, (960, 540)))\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Show Contours***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_img = cv2.cvtColor(thresh_img, cv2.COLOR_BGR2RGB)\n",
    "cv2.drawContours(print_img, top_25_contours, -1, (0, 255, 0), 3)\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "fontScale = 10\n",
    "color = (255, 0, 0)\n",
    "thickness = 5\n",
    "for i, place in enumerate(sorted_y):  \n",
    "    cv2.putText(print_img, str(i), (place[1] + 10,place[2] + 10), font, \n",
    "                   fontScale, color, thickness, cv2.LINE_AA)\n",
    "imS = cv2.resize(print_img, (960, 540)) \n",
    "cv2.imshow('Contours', imS)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattener(image, pts, w, h):\n",
    "    \"\"\"Flattens an image of a card into a top-down 200x300 perspective.\n",
    "    Returns the flattened, re-sized, grayed image.\n",
    "    See www.pyimagesearch.com/2014/08/25/4-point-opencv-getperspective-transform-example/\"\"\"\n",
    "    temp_rect = np.zeros((4,2), dtype = \"float32\")\n",
    "    \n",
    "    s = np.sum(pts, axis = 2)\n",
    "\n",
    "    tl = pts[np.argmin(s)]\n",
    "    br = pts[np.argmax(s)]\n",
    "\n",
    "    diff = np.diff(pts, axis = -1)\n",
    "    tr = pts[np.argmin(diff)]\n",
    "    bl = pts[np.argmax(diff)]\n",
    "\n",
    "    # Need to create an array listing points in order of\n",
    "    # [top left, top right, bottom right, bottom left]\n",
    "    # before doing the perspective transform\n",
    "\n",
    "    if w <= 0.8*h: # If card is vertically oriented\n",
    "        temp_rect[0] = tl\n",
    "        temp_rect[1] = tr\n",
    "        temp_rect[2] = br\n",
    "        temp_rect[3] = bl\n",
    "\n",
    "    if w >= 1.2*h: # If card is horizontally oriented\n",
    "        temp_rect[0] = bl\n",
    "        temp_rect[1] = tl\n",
    "        temp_rect[2] = tr\n",
    "        temp_rect[3] = br\n",
    "\n",
    "    # If the card is 'diamond' oriented, a different algorithm\n",
    "    # has to be used to identify which point is top left, top right\n",
    "    # bottom left, and bottom right.\n",
    "    \n",
    "    if w > 0.8*h and w < 1.2*h: #If card is diamond oriented\n",
    "        # If furthest left point is higher than furthest right point,\n",
    "        # card is tilted to the left.\n",
    "        if pts[1][0][1] <= pts[3][0][1]:\n",
    "            # If card is titled to the left, approxPolyDP returns points\n",
    "            # in this order: top right, top left, bottom left, bottom right\n",
    "            temp_rect[0] = pts[1][0] # Top left\n",
    "            temp_rect[1] = pts[0][0] # Top right\n",
    "            temp_rect[2] = pts[3][0] # Bottom right\n",
    "            temp_rect[3] = pts[2][0] # Bottom left\n",
    "\n",
    "        # If furthest left point is lower than furthest right point,\n",
    "        # card is tilted to the right\n",
    "        if pts[1][0][1] > pts[3][0][1]:\n",
    "            # If card is titled to the right, approxPolyDP returns points\n",
    "            # in this order: top left, bottom left, bottom right, top right\n",
    "            temp_rect[0] = pts[0][0] # Top left\n",
    "            temp_rect[1] = pts[3][0] # Top right\n",
    "            temp_rect[2] = pts[2][0] # Bottom right\n",
    "            temp_rect[3] = pts[1][0] # Bottom left\n",
    "            \n",
    "        \n",
    "    maxWidth = 200\n",
    "    maxHeight = 300\n",
    "\n",
    "    # Create destination array, calculate perspective transform matrix,\n",
    "    # and warp card image\n",
    "    dst = np.array([[0,0],[maxWidth-1,0],[maxWidth-1,maxHeight-1],[0, maxHeight-1]], np.float32)\n",
    "    M = cv2.getPerspectiveTransform(temp_rect,dst)\n",
    "    warp = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\n",
    "    warp = cv2.cvtColor(warp,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        \n",
    "\n",
    "    return warp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Words on Each Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_words(top_25_sorted):\n",
    "    words = []\n",
    "    for cont in top_25_sorted:\n",
    "        peri = cv2.arcLength(cont,True)\n",
    "        approx = cv2.approxPolyDP(cont,0.01*peri,True)\n",
    "        pts = np.float32(approx)\n",
    "        corner_pts = pts\n",
    "\n",
    "        x,y,w,h = cv2.boundingRect(cont)\n",
    "        width, height = w, h\n",
    "\n",
    "        average = np.sum(pts, axis=0)/len(pts)\n",
    "        cent_x = int(average[0][0])\n",
    "        cent_y = int(average[0][1])\n",
    "        center = [cent_x, cent_y]\n",
    "\n",
    "        warp = cv2.rotate(flattener(word_board, pts, w, h),cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        cropped_img = warp[warp.shape[0]//2 + 20: warp.shape[0]-20, 20:warp.shape[1]-20]\n",
    "        blur = cv2.GaussianBlur(cropped_img, (3,3), 0)\n",
    "        contrast = cv2.convertScaleAbs(blur, alpha=1.3, beta=0)\n",
    "        thresh = cv2.threshold(contrast, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "        words.append(pytesseract.image_to_string(thresh, lang='eng', config='--psm 6').strip())\n",
    "        # cv2.imshow('warp', warp)\n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.imshow('warp',cropped_img)\n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.imshow('Contrast', contrast)\n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.imshow('thresh', thresh)\n",
    "        # cv2.waitKey(0)\n",
    "    return words\n",
    "\n",
    "words = find_words(top_25_sorted)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clue Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = datapath('/assets/glove.6B.100d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec('assets/glove.6B.100d.txt', word2vec_glove_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Word2Vec Method***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [\"missile\", \"father\", \"spring\", \"soul\", \"film\", \"cast\"]\n",
    "bad = [\"piano\", \"sub\", \"rock\", \"ham\"]\n",
    "\n",
    "filtered_answers = []\n",
    "filtered_bad = []\n",
    "\n",
    "for word in answers:\n",
    "    try:\n",
    "        _ = model[word]\n",
    "        filtered_answers.append(word)\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "for word in bad:\n",
    "    try:\n",
    "        _ = model[word]\n",
    "        filtered_bad.append(word)\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "model.most_similar(positive=filtered_answers, negative=filtered_bad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cosine Similarity Method***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine approach https://jsomers.net/glove-codenames/\n",
    "\n",
    "embeddings = {}\n",
    "with open(\"/content/glove.6B.100d.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings[word] = vector\n",
    "\n",
    "def distance(word, reference):\n",
    "    return spatial.distance.cosine(embeddings[word], embeddings[reference])\n",
    "\n",
    "def closest_words(reference):\n",
    "    return sorted(embeddings.keys(), key=lambda w: distance(w, reference))\n",
    "\n",
    "def goodness(word, answers, bad):\n",
    "    if word in answers + bad: return -999\n",
    "    return sum([distance(word, b) for b in bad]) - 4.0 * sum([distance(word, a) for a in answers])\n",
    "\n",
    "def minimax(word, answers, bad):\n",
    "    if word in answers + bad: return -999\n",
    "    return min([distance(word, b) for b in bad]) - max([distance(word, a) for a in answers])\n",
    "\n",
    "def candidates(answers, bad, size=100):\n",
    "    best = sorted(embeddings.keys(), key=lambda w: -1 * goodness(w, answers, bad))\n",
    "    res = [(str(i + 1), \"{0:.2f}\".format(minimax(w, answers, bad)), w) for i, w in enumerate(sorted(best[:250], key=lambda w: -1 * minimax(w, answers, bad))[:size])]\n",
    "    return [(\". \".join([c[0], c[2]]) + \" (\" + c[1] + \")\") for c in res]\n",
    "\n",
    "def grouper(n, iterable, fillvalue=None):\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip_longest(fillvalue=fillvalue, *args)\n",
    "\n",
    "def tabulate(data):\n",
    "    data = list(grouper(10, data))\n",
    "    return HTML(pd.DataFrame(data).to_html(index=False, header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [\"happy\", \"cup\", \"dog\"]\n",
    "bad = [\"table\"]\n",
    "\n",
    "tabulate(candidates(answers, bad))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-prac-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
